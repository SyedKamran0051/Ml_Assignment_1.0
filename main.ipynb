{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree.ElementTree import fromstring\n",
    "from pyspark.sql import SparkSession\n",
    "from Preprocessing.data_manipulation import DataPreparation\n",
    "from Transformers.data_aggregation import AggregateData\n",
    "from Transformers.impute_mean import ImputePrice\n",
    "from Transformers.negative_sales import NegativeSales\n",
    "from Transformers.logtransformer import Log\n",
    "from pyspark.ml import Pipeline\n",
    "from Transformers.scalar_na_filler import ScallerNAFiller\n",
    "from Transformers.lagtransformer import Lags\n",
    "from Transformers.test_train_split import Split\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from Evaluator.Mape import MAPE\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from Estimator.random_forest import RandomForest\n",
    "from Evaluator.Mape import MAPE\n",
    "from Estimator.XGBoost import XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Spark Session\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"project_spark\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"15g\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = Data Frame \n",
    "data = DataPreparation()\n",
    "df_m5 = data.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------+--------------------+-------+------+--------+-----+----------+---------+----+-----+----+-------------+------------+------------+------------+-------+-------+-------+----------+\n",
      "|store_id|    item_id|wm_yr_wk|                  id|dept_id|cat_id|state_id|sales|      date|  weekday|wday|month|year| event_name_1|event_type_1|event_name_2|event_type_2|snap_CA|snap_TX|snap_WI|sell_price|\n",
      "+--------+-----------+--------+--------------------+-------+------+--------+-----+----------+---------+----+-----+----+-------------+------------+------------+------------+-------+-------+-------+----------+\n",
      "|    WI_1|FOODS_1_001|   11507|FOODS_1_001_WI_1_...|FOODS_1| FOODS|      WI|    0|2015-03-14| Saturday|   1|    3|2015|         null|        null|        null|        null|      0|      0|      1|      2.24|\n",
      "|    WI_1|FOODS_1_001|   11507|FOODS_1_001_WI_1_...|FOODS_1| FOODS|      WI|    0|2015-03-15|   Sunday|   2|    3|2015|         null|        null|        null|        null|      0|      1|      1|      2.24|\n",
      "|    WI_1|FOODS_1_001|   11507|FOODS_1_001_WI_1_...|FOODS_1| FOODS|      WI|    0|2015-03-16|   Monday|   3|    3|2015|         null|        null|        null|        null|      0|      0|      0|      2.24|\n",
      "|    WI_1|FOODS_1_001|   11507|FOODS_1_001_WI_1_...|FOODS_1| FOODS|      WI|    1|2015-03-17|  Tuesday|   4|    3|2015|StPatricksDay|    Cultural|        null|        null|      0|      0|      0|      2.24|\n",
      "|    WI_1|FOODS_1_001|   11507|FOODS_1_001_WI_1_...|FOODS_1| FOODS|      WI|    0|2015-03-18|Wednesday|   5|    3|2015|         null|        null|        null|        null|      0|      0|      0|      2.24|\n",
      "+--------+-----------+--------+--------------------+-------+------+--------+-----+----------+---------+----+-----+----+-------------+------------+------------+------------+-------+-------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_m5 = data.filter_store(df_m5, \"WI_1\")\n",
    "df_m5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store_id',\n",
       " 'item_id',\n",
       " 'wm_yr_wk',\n",
       " 'id',\n",
       " 'dept_id',\n",
       " 'cat_id',\n",
       " 'state_id',\n",
       " 'sales',\n",
       " 'date',\n",
       " 'weekday',\n",
       " 'wday',\n",
       " 'month',\n",
       " 'year',\n",
       " 'event_name_1',\n",
       " 'event_type_1',\n",
       " 'event_name_2',\n",
       " 'event_type_2',\n",
       " 'snap_CA',\n",
       " 'snap_TX',\n",
       " 'snap_WI',\n",
       " 'sell_price']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_m5.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiating different Transformers #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputeNegativePrice = ImputePrice()\n",
    "negativeSales = NegativeSales(column=\"sales\")\n",
    "aggregate = AggregateData(columns=[\"store_id\", \"dept_id\", \"year\", \"month\"],\n",
    "                                expressions={\"sales\": \"sum\",\n",
    "                                \"sell_price\": \"avg\",\n",
    "                                \"snap_WI\": \"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transform = Log(inputCols=[\"sales\",\"sell_price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_feature_transform = Lags(lags=[1,2,3], target=\"sales\", partitionBy=[\"store_id\",\"dept_id\"], orderBy=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "na_filler = ScallerNAFiller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "storeIndexer = StringIndexer(inputCol=\"store_id\", outputCol=\"store_id_index\")\n",
    "yearIndexer = StringIndexer(inputCol=\"year\", outputCol=\"year_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputColumns = ['month',\n",
    "        'sell_price',\n",
    "        'snap_WI',\n",
    "        'lag_1',\n",
    "        'lag_2',\n",
    "        'lag_3',\n",
    "        'store_id_index',\n",
    "        'year_index']\n",
    "assembler = VectorAssembler(inputCols=inputColumns, \n",
    "                                    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Preprocessing/FeatureEngineering Pipeline Complete #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [54], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m transformed \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mimputeNegativePrice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnegativeSales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mlog_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag_feature_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstoreIndexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myearIndexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_filler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massembler\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_m5\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(df_m5)\n",
      "File \u001b[1;32m~\\spark3\\python\\pyspark\\ml\\base.py:131\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\spark3\\python\\pyspark\\ml\\pipeline.py:109\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[0;32m    108\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 109\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[0;32m    110\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32m~\\spark3\\python\\pyspark\\ml\\base.py:131\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    130\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 131\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    132\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[1;32m~\\spark3\\python\\pyspark\\ml\\wrapper.py:321\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[1;32m--> 321\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    322\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    323\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\spark3\\python\\pyspark\\ml\\wrapper.py:318\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[0;32m    311\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[39m:return: fitted Java model\u001b[39;00m\n\u001b[0;32m    316\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32m~\\spark3\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py:1284\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[0;32m   1279\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1281\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1282\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1284\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1285\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1286\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1288\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\spark3\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py:1014\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1012\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[0;32m   1013\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[0;32m   1015\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[0;32m   1016\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\spark3\\python\\lib\\py4j-0.10.8.1-src.zip\\py4j\\java_gateway.py:1181\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1177\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JNetworkError(\n\u001b[0;32m   1178\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mError while sending\u001b[39m\u001b[39m\"\u001b[39m, e, proto\u001b[39m.\u001b[39mERROR_ON_SEND)\n\u001b[0;32m   1180\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m   1182\u001b[0m     logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[0;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m answer\u001b[39m.\u001b[39mstartswith(proto\u001b[39m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[1;32mc:\\Users\\muhammad.kamran01\\python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "transformed = Pipeline(stages=[imputeNegativePrice, negativeSales, aggregate, \n",
    "                       log_transform, lag_feature_transform, storeIndexer, yearIndexer, na_filler, assembler]).fit(df_m5).transform(df_m5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Training #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spliting = DataPreparation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = spliting.train_test_split(transformed, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfModel = RandomForest(featuresCol=\"features\", labelCol=\"sales\").fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['store_id',\n",
       " 'dept_id',\n",
       " 'year',\n",
       " 'month',\n",
       " 'sell_price',\n",
       " 'snap_WI',\n",
       " 'sales',\n",
       " 'lag_1',\n",
       " 'lag_2',\n",
       " 'lag_3',\n",
       " 'store_id_index',\n",
       " 'year_index',\n",
       " 'features',\n",
       " 'prediction']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfModel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-----------------+-----------------+\n",
      "|store_id|year|            sales|       prediction|\n",
      "+--------+----+-----------------+-----------------+\n",
      "|    WI_1|2015|8.448485993406447|8.299308821829937|\n",
      "|    WI_1|2015|8.445052513638554|8.486747116006214|\n",
      "|    WI_1|2015|8.589699882202986|8.499727021409225|\n",
      "|    WI_1|2015| 8.46695197497949|8.499727021409225|\n",
      "|    WI_1|2015|8.495765244002618|8.499727021409225|\n",
      "|    WI_1|2015|8.473868066677865|8.682446834072161|\n",
      "|    WI_1|2015|8.536799721055155|8.499727021409225|\n",
      "|    WI_1|2015|8.518991573357617|8.499727021409225|\n",
      "|    WI_1|2015|8.377011160816375|8.499727021409225|\n",
      "|    WI_1|2015|8.399085102935908| 8.50781892320288|\n",
      "+--------+----+-----------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfModel.select([\"store_id\",\"year\",\"sales\",\"prediction\"]).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator_mape = MAPE(predictionCol=\"prediction\", labelCol=\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = Evaluator_mape.evaluate(rfModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation matrix Score for RFMODEL: 0.01341233405185784\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation matrix Score for RFMODEL:\", evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST ESTIMATOR IMPLEMENTATION #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- store_id: string (nullable = true)\n",
      " |-- dept_id: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- sell_price: double (nullable = false)\n",
      " |-- snap_WI: long (nullable = true)\n",
      " |-- sales: double (nullable = false)\n",
      " |-- lag_1: double (nullable = false)\n",
      " |-- lag_2: double (nullable = false)\n",
      " |-- lag_3: double (nullable = false)\n",
      " |-- store_id_index: double (nullable = false)\n",
      " |-- year_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBoostModel = XGBoost(inputCols=inputColumns, labelCol=\"sales\").fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muhammad.kamran01\\spark3\\python\\pyspark\\sql\\session.py:505: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "pred = XGBoostModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+------------------+------------------+\n",
      "|store|year|month|        prediction|            actual|\n",
      "+-----+----+-----+------------------+------------------+\n",
      "|  0.0| 5.0|    1|  9.59622573852539| 9.607033787697222|\n",
      "|  0.0| 5.0|    2| 9.203985214233398| 9.190647738630446|\n",
      "|  0.0| 5.0|    3| 9.421991348266602| 9.763363044441961|\n",
      "|  0.0| 5.0|    4| 9.182626724243164| 9.285726098882073|\n",
      "|  0.0| 5.0|    1| 8.666375160217285| 9.727525729694754|\n",
      "|  0.0| 5.0|    2| 7.343536853790283| 8.441607204459642|\n",
      "|  0.0| 5.0|    3| 8.536872863769531| 9.754697478950966|\n",
      "|  0.0| 5.0|    4| 8.854217529296875| 8.931155429778348|\n",
      "|  0.0| 5.0|    1|  7.92768669128418| 8.516793111394898|\n",
      "|  0.0| 5.0|    2| 8.364577293395996| 9.826822467600643|\n",
      "|  0.0| 5.0|    3| 9.656170845031738| 9.353747835270912|\n",
      "|  0.0| 5.0|    4| 7.531565189361572|6.7226297948554485|\n",
      "|  0.0| 5.0|    1|10.656360626220703| 9.920393832602487|\n",
      "|  0.0| 5.0|    2|11.167980194091797| 10.82017820443161|\n",
      "|  0.0| 5.0|    3|10.162330627441406| 9.053335623166017|\n",
      "|  0.0| 5.0|    4|10.025012016296387| 9.899077844047428|\n",
      "|  0.0| 5.0|    1|10.113859176635742| 8.432724034789787|\n",
      "|  0.0| 5.0|    2| 8.814543724060059|10.795977274224331|\n",
      "|  0.0| 5.0|    3| 8.479259490966797| 9.130864170474386|\n",
      "|  0.0| 5.0|    4| 9.104218482971191| 9.884508585938326|\n",
      "+-----+----+-----+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluator_mape_xgBoost = MAPE(predictionCol=\"prediction\", labelCol=\"actual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_xgb = Evaluator_mape_xgBoost.evaluate(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation matrix Score for XGFMODEL: 0.13241363950334317\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluation matrix Score for XGFMODEL:\", evaluation_xgb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b93898b9253904288df13a2bfa30ad60602e1c62b3892988ef0a02f0b401d94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
